# Dockerfile for vLLM with qr-sampler plugin baked in.
#
# Installs qr-sampler (with gRPC support) at build time, producing a
# reproducible, cacheable image. The plugin is discovered automatically
# by vLLM via the vllm.logits_processors entry point.
#
# Build (from repo root):
#   docker build -f examples/docker/Dockerfile.vllm -t qr-vllm .
#
# Run with a deployment profile (recommended):
#   cd deployments/urandom
#   docker compose up --build
#
# Run standalone (system entropy fallback):
#   docker run --gpus 1 -p 8000:8000 \
#     -e HF_MODEL=Qwen/Qwen2.5-1.5B-Instruct \
#     -e HF_TOKEN=<your-token> \
#     qr-vllm

FROM vllm/vllm-openai:latest

# Prevent Python from writing .pyc files and enable unbuffered output.
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /opt/qr-sampler

# Copy only pyproject.toml first to leverage Docker layer caching —
# pip install will only re-run if dependencies change.
COPY pyproject.toml ./

# Copy source tree.
COPY src/ ./src/

# Install qr-sampler (includes gRPC support).
RUN pip install --no-cache-dir .

# Default qr-sampler configuration: system entropy (works without
# any external server). Override via environment variables or an
# --env-file from a deployment profile.
ENV QR_ENTROPY_SOURCE_TYPE=system

# vLLM defaults — override via environment variables.
ENV HF_MODEL=Qwen/Qwen2.5-1.5B-Instruct

EXPOSE 8000

# Reset ENTRYPOINT set by vllm/vllm-openai base image so CMD runs as-is.
ENTRYPOINT []

# Start vLLM. The qr-sampler plugin is auto-discovered via entry points.
# Shell form so environment variables are resolved at runtime.
CMD vllm serve ${HF_MODEL} --host 0.0.0.0 --port 8000 --dtype half --max-model-len 8096 --gpu-memory-utilization 0.80
