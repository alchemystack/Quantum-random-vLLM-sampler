# Docker Compose for running qr-sampler with an entropy server and vLLM.
#
# This sets up the full stack:
#   1. entropy-server: gRPC entropy source (os.urandom by default)
#   2. vllm: vLLM inference server with qr-sampler plugin installed
#
# Quick start:
#   docker compose up
#
# Use a different entropy server:
#   ENTROPY_SERVER=timing_noise_server.py docker compose up
#
# Use bidi streaming for lowest latency:
#   QR_GRPC_MODE=bidi_streaming docker compose up
#
# Notes:
#   - The vLLM service requires a GPU. Remove 'deploy.resources' if testing
#     on CPU (vLLM will fall back to CPU mode for some models).
#   - Adjust HF_MODEL to the Hugging Face model you want to serve.
#   - Both services share a Docker network so the entropy server is
#     reachable at 'entropy-server:50051' from vLLM.

services:
  # ------------------------------------------------------------------
  # Entropy server â€” provides random bytes over gRPC
  # ------------------------------------------------------------------
  entropy-server:
    build:
      context: ../..
      dockerfile: examples/docker/Dockerfile.entropy-server
    ports:
      - "${ENTROPY_PORT:-50051}:50051"
    environment:
      # Which example server to run (default: simple_urandom_server.py).
      ENTROPY_SERVER: "${ENTROPY_SERVER:-simple_urandom_server.py}"
    restart: unless-stopped
    healthcheck:
      test: >-
        python -c "
        import grpc;
        ch = grpc.insecure_channel('localhost:50051');
        grpc.channel_ready_future(ch).result(timeout=3)
        "
      interval: 30s
      timeout: 5s
      start_period: 5s
      retries: 3

  # ------------------------------------------------------------------
  # vLLM inference server with qr-sampler plugin
  # ------------------------------------------------------------------
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "${VLLM_PORT:-8000}:8000"
    volumes:
      # Mount the qr-sampler source so it can be installed as a plugin.
      - ../../src:/opt/qr-sampler/src:ro
      - ../../pyproject.toml:/opt/qr-sampler/pyproject.toml:ro
    environment:
      # --- qr-sampler configuration ---
      QR_ENTROPY_SOURCE_TYPE: "quantum_grpc"
      QR_GRPC_SERVER_ADDRESS: "entropy-server:50051"
      QR_GRPC_MODE: "${QR_GRPC_MODE:-unary}"
      QR_GRPC_TIMEOUT_MS: "5000"
      QR_FALLBACK_MODE: "system"
      QR_SAMPLE_COUNT: "20480"
      QR_TEMPERATURE_STRATEGY: "${QR_TEMPERATURE_STRATEGY:-fixed}"
      QR_FIXED_TEMPERATURE: "${QR_FIXED_TEMPERATURE:-0.7}"
      QR_TOP_K: "${QR_TOP_K:-50}"
      QR_TOP_P: "${QR_TOP_P:-0.9}"
      QR_LOG_LEVEL: "${QR_LOG_LEVEL:-summary}"

      # --- Hugging Face model to serve ---
      HF_MODEL: "${HF_MODEL:-meta-llama/Llama-3.2-1B}"
      HF_TOKEN: "${HF_TOKEN}"
    # Install qr-sampler plugin then start vLLM.
    command: >-
      bash -c "
        pip install /opt/qr-sampler[grpc] &&
        python -m vllm.entrypoints.openai.api_server
          --model ${HF_MODEL:-meta-llama/Llama-3.2-1B}
          --host 0.0.0.0
          --port 8000
      "
    depends_on:
      entropy-server:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
